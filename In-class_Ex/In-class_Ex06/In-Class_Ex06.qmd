---
title: "In-Class Exercise 6: Geographical Segmentation with Spatially Constrained Clustering Techniques"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      eval = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      freeze = TRUE)
```

# Overview

In this hands-on exercise, you will gain hands-on experience on how to delineate homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely: - hierarchical cluster analysis; and - spatially constrained cluster analysis.

# Learning Outcome

By the end of this hands-on exercise, we will able to: - convert GIS polygon data into R's simple feature data.frame by using appropriate functions of sf package of R; - convert simple feature data.frame into R's SpatialPolygonDataFrame object by using appropriate sf of package of R; - perform custer analysis by using hclust() of Base R; - perform spatially constrained cluster analysis using skater() of Base R; and - visualise the analysis output by using ggplot2 and tmap package.

# The analytical question

In geobusiness and spatial policy, it is a common practice to delineate the market or planning area into homogeneous regions by using multivariate data. In this hands-on exercise, we are interested to delineate Shan State, Myanmar into homogeneous regions by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.

# Packages

The R packages needed for this exercise are as follows: 1. Spatial data handling: sf, sp and spdep 2. Attribute data handling: tidyverse, especially readr, ggplot2 and dplyr 3. Choropleth mapping: tmap 4. Multivariate data visualisation and analysis: coorplot, ggpubr, and heatmaply 5. Cluster analysis: cluster, ClustGeo

```{r}
pacman::p_load(sp, spdep, tmap, sf, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, GGally)
```

# Data

Two data sets will be used: 1. Myanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features. 2. Shan-ICT.csv: This is an extract of [The 2014 Myanmar Population and Housing Census](Myanmarhttps://myanmar.unfpa.org/en/publications/2014-population-and-housing-census-myanmar-data-sheet) at the township level.

## Importing Geospatial Data

We will use st_read to save the geospatial data to a variable called shan_sf, and use summary() to get a feel of the dataset

```{r}
shan_sf <- st_read(dsn = "data/geospatial", 
                    layer = "myanmar_township_boundaries") |>
                    filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) |>
                    select(c(2:7))
summary(shan_sf)
```

## Importing Aspatial Data

For the aspatial data, we will use read_csv()

```{r}
ict <- read_csv ("data/aspatial/Shan-ICT.csv")
summary(ict)
```

The unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.

In order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
summary(ict_derived)
```

## EDA

### Histogram

Histogram is useful to know the distribution of the data.

Here, we will see the distribution of the original RADIO

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

And here we will see the distribution of the derived RADIO_PR

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Here is all the histograms for the derived variables

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)
```

### Boxplot

Boxplot is useful to detect outliers

We can look at the RADIO variable

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

Here we are looking at the derived RADIO_PR

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

### Choropleth

#### Joining geospatial and aspatial data

Before we can make a choropleth map, we need to combine the geospatial and aspatial data. We can use left_join() to do so, based on the TS_PCODE column

```{r}
shan_sf <- left_join(shan_sf, 
                     ict_derived, by=c("TS_PCODE"="TS_PCODE"))
  
write_rds(shan_sf, "data/rds/shan_sf.rds")
```

The combined data is saved into an rds file, which we can call with read_rds()

```{r}
shan_sf <- read_rds("data/rds/shan_sf.rds")
```

#### Preparing choropleth

We can use qtm() to quickly make a choropleth

```{r}
qtm(shan_sf, "RADIO_PR")
```

In order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map)

```{r}
TT_HOUSEHOLDS.map <- tm_shape(shan_sf) + 
  tm_fill(col = "TT_HOUSEHOLDS",
          n = 5,
          style = "jenks", 
          title = "Total households") + 
  tm_borders(alpha = 0.5) 

RADIO.map <- tm_shape(shan_sf) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

Notice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.

Now let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.

```{r}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

# Correlation Analysis

Before we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated. To test that, we can use corrplot.mixed() from corrplot package

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

We can see that COMPUTER_PR and INTERNET_PR is highly correlated

# Hierarchy Cluster Analysis

Hierarchical cluster analysis consists of these steps:

## 1. Extracting Cluster Analysis

The code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.

```{r}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars,10)
```

The final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.

We need to change the rows by township name instead of row number, so that we can call the rows by the township name

```{r}
row.names(cluster_vars) <- cluster_vars$"TS.x"

shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

## 2. Data Standardisation

### Min-Max Standardisation

normalize() uses min-max method to standardize the data. The min-max standardized variables will then turn to be between 0-1

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

### Z-Score Standardisation

scale() will standardizes data using Z-score methods. The mean and standard deviation of the Z-score standardized variables will be 0 and 1 respectively. Z-score standardisation should only be used if we are sure that the data is normally distributed

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

### Visualising Standardised Clustering Variables

It is good practice to visualise the distribution.

With histogram:

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

With density plot:

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

## 3. Computing Proximity Matrix

We will use dist() from R to compute the proximity matrix. dist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
proxmat
```

## 4. Computing Hieararchical Clustering

We will use hclust() to compute hieararchical clustering function. hclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
plot(hclust_ward, cex = 0.6)
```

# Selecting The Optimal Clustering Algorithm

One of the challenge in hierarchical clustering is identifying stronger clustering structures. Thus, we can use agnes() from cluster package, that functions like hclus() while also getting the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure)

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

From the result above, Ward's method provide the strongest clustering, thus we will proceed with Ward's

# Determining Optimal Clusters

Another challenge is determining the optimal clusters to retain. The three commonly used methods are Elbow method, Average Silhouette method, and Gap Statistic method

## Gap Statistic Method

The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.We can use clusGap() from the cluster package

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

the hcut function used inside is taken from factoextra. Also from factoextra, we can use fvis_gap_stat() to visualize the plot

```{r}
fviz_gap_stat(gap_stat)
```

Based on the plot, the recommended number of cluster is 1 (highest Gap statistic).

# Interpreting The Dendograms

In the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.

The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.

We can use rect.hclust() to draw a dendogram with a border around the selected clusters

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, 
            border = 2:5)
```

# Visually-driven Hierarchical Clustering Analysis

We will use the heatmaply package

## Transform Data to Matrix

To make our heatmap, we need to convert the data into a matrix

```{r}
shan_ict_mat <- data.matrix(shan_ict)
```

## Plotting With heatmaply()

```{r}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )
```

# Mapping The Formed Clusters

With closed examination of the dendragram above, we have decided to retain six clusters.

cutree() of R Base will be used in the code chunk below to derive a 6-cluster model.

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
```

We need to append the groups into shan_sf to visualize it

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) |>
  rename(`CLUSTER`=`as.matrix.groups.`)

qtm(shan_sf_cluster, "CLUSTER")
```

# Spatially Constrained Clustering: SKATER

## 1. Compute Neighbour List

We will use poly2nd() to compute neighbours list

```{r}
shan.nb <- poly2nb(shan_sf)
summary(shan.nb)
```

## 2. Computing Minimum Spanning Tree

First, we calculate the cost of each edge with nbcosts()

```{r}
lcosts <- nbcosts(shan.nb, shan_ict)
```

Next, we incorporate these costs into a weights object with nb2listw(). We set style as B to make sure the cost values are not row-standardised

```{r}
shan.w <- nb2listw(shan.nb, 
                   lcosts, 
                   style="B")
summary(shan.w)
```

Based on the weights, the spanning tree is computed by mstree()

```{r}
shan.mst <- mstree(shan.w)
head(shan.mst)
```

## 3. Comparing Spatially Constrained Clusters With SKATER Method

```{r}
clust6 <- spdep::skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
```

The skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters. The result of the skater() is an object of class skater. We can examine its contents by using the code chunk below.

```{r}
str(clust6)
```

We can check the cluster assignment

```{r}
ccs6 <- clust6$groups
ccs6
```

And find out how many observations are in each cluster

```{r}
table(ccs6)
```

## 5. Visualising The CLusters In Choropleth Map

```{r}
groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster, "SP_CLUSTER")
```

For easy comparison:

```{r}
hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 

shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```

```{r}
plot(st_geometry(shan_sf),
     border=grey(.5))

pts <- st_coordinates(st_centroid(shan_sf))

plot(shan.nb,
     pts,
     col="blue",
     add=TRUE)
```

# Spatially Constrained Clustering: ClustGeo Method

ClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints. The algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between \[0, 1\].

## Ward-like Hierarchical Clustering: ClustGeo

To perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix

```{r}
nongeo_cluster <- hclustgeo(proxmat)
plot(nongeo_cluster, cex = 0.5)
rect.hclust(nongeo_cluster, 
            k = 6, 
            border = 2:5)
```

## Mapping The Formed Clusters

```{r}
groups <- as.factor(cutree(nongeo_cluster, k=6))
shan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)

qtm(shan_sf_ngeo_cluster, "CLUSTER")
```

# Spatially Constrained Hierarchical Clusering

Before we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.

```{r}
dist <- st_distance(shan_sf, shan_sf)
distmat <- as.dist(dist)
cr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)
```

as.dits() converts the data frame into matrix, and choicealpha() will determine the value for mixing parameter alpha.

Based on the graph above, we will use alpha=0.3

```{r}
clustG <- hclustgeo(proxmat, distmat, alpha = 0.3)
groups <- as.factor(cutree(clustG, k=6))
shan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
qtm(shan_sf_Gcluster, "CLUSTER")
```

# Visual Interpretation of Clusters

## Individual Clustering Variable

```{r}
ggplot(data = shan_sf_ngeo_cluster,
       aes(x = CLUSTER, y = RADIO_PR)) +
  geom_boxplot()
```

The boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household

## Multivariate

We can use parallel coordinate plot to reveal clustering variables. We will use ggparcoord() from Gally

```{r}
ggparcoord(data = shan_sf_ngeo_cluster, 
           columns = c(17:21), 
           scale = "globalminmax",
           alphaLines = 0.2,
           boxplot = TRUE, 
           title = "Multiple Parallel Coordinates Plots of ICT Variables by Cluster") +
  facet_grid(~ CLUSTER) + 
  theme(axis.text.x = element_text(angle = 30))
```

The parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.

We can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.In the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.

```{r}
shan_sf_ngeo_cluster %>% 
  st_set_geometry(NULL) %>%
  group_by(CLUSTER) %>%
  summarise(mean_RADIO_PR = mean(RADIO_PR),
            mean_TV_PR = mean(TV_PR),
            mean_LLPHONE_PR = mean(LLPHONE_PR),
            mean_MPHONE_PR = mean(MPHONE_PR),
            mean_COMPUTER_PR = mean(COMPUTER_PR))
```
